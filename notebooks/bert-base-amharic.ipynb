{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SrqFQV7zqICK",
      "metadata": {
        "id": "SrqFQV7zqICK"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    DataCollatorForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06a67a49",
      "metadata": {
        "id": "06a67a49"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c09d8f6",
      "metadata": {
        "id": "5c09d8f6"
      },
      "outputs": [],
      "source": [
        "def load_conll_file(file_path):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    tokens = []\n",
        "    tags = []\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if tokens:\n",
        "                    sentences.append(tokens)\n",
        "                    labels.append(tags)\n",
        "                    tokens = []\n",
        "                    tags = []\n",
        "            else:\n",
        "                splits = line.split()\n",
        "                if len(splits) == 2:\n",
        "                    token, tag = splits\n",
        "                    tokens.append(token)\n",
        "                    tags.append(tag)\n",
        "\n",
        "    # Append last sentence if file doesn't end with newline\n",
        "    if tokens:\n",
        "        sentences.append(tokens)\n",
        "        labels.append(tags)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "conll_path = \"/content/labeled_data.txt\"\n",
        "sentences, ner_tags = load_conll_file(conll_path)\n",
        "\n",
        "print(f\"âœ… Loaded {len(sentences)} sentences\")\n",
        "print(\"ðŸ§¾ Sample:\")\n",
        "for tok, tag in zip(sentences[0], ner_tags[0]):\n",
        "    print(f\"{tok:10} {tag}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "529d90ea",
      "metadata": {
        "id": "529d90ea"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Define label list in challenge order\n",
        "label_list = [\n",
        "    \"O\",\n",
        "    \"B-Product\", \"I-Product\",\n",
        "    \"B-PRICE\", \"I-PRICE\",\n",
        "    \"B-LOC\", \"I-LOC\"\n",
        "]\n",
        "\n",
        "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
        "id_to_label = {i: label for label, i in label_to_id.items()}\n",
        "\n",
        "# Prepare data\n",
        "data = [{\"tokens\": tokens, \"ner_tags\": [label_to_id[tag] for tag in tags]}\n",
        "        for tokens, tags in zip(sentences, ner_tags)]\n",
        "\n",
        "dataset = Dataset.from_list(data)\n",
        "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = dataset[\"train\"]\n",
        "eval_dataset = dataset[\"test\"]\n",
        "\n",
        "print(\"âœ… Dataset prepared:\")\n",
        "train_dataset[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaHhje7EoMQl",
      "metadata": {
        "collapsed": true,
        "id": "eaHhje7EoMQl"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"Davlan/bert-base-multilingual-cased-finetuned-amharic\"\n",
        "run_name = \"bert-amharic-ner\"\n",
        "output_dir = \"/content/bert-base-amharic\"\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "145fc721",
      "metadata": {
        "collapsed": true,
        "id": "145fc721"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=len(label_list),\n",
        "    id2label=id_to_label,\n",
        "    label2id=label_to_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41748623",
      "metadata": {
        "collapsed": true,
        "id": "41748623"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True,\n",
        "        is_split_into_words=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=True\n",
        "    )\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                # Assign -100 to subword pieces so they are ignored in loss\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Apply to train and eval sets\n",
        "tokenized_train = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "tokenized_eval = eval_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "print(\"âœ… Tokenization complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25cd40b5",
      "metadata": {
        "id": "25cd40b5"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "seqeval = evaluate.load(\"seqeval\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [\n",
        "        [id_to_label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [id_to_label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dde9cfe5",
      "metadata": {
        "id": "dde9cfe5"
      },
      "outputs": [],
      "source": [
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    run_name=run_name,\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=6,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=f\"{output_dir}/logs\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fba1a734",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "fba1a734",
        "outputId": "e8777e16-a313-49ae-c641-f346111738af"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AxUhqqJMrabC",
      "metadata": {
        "id": "AxUhqqJMrabC"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"/content/bert-base-amharic\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u9lwP1E-uXWG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9lwP1E-uXWG",
        "outputId": "79bc8db1-881e-42f2-bbec-4b09b606b31f"
      },
      "outputs": [],
      "source": [
        "tokenizer.save_pretrained(\"/content/bert-base-amharic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9WivxiabzVbf",
      "metadata": {
        "id": "9WivxiabzVbf"
      },
      "outputs": [],
      "source": [
        "!zip -r bert-base-amharic-model.zip /content/bert-base-amharic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z2vgYvfD3ZuU",
      "metadata": {
        "id": "Z2vgYvfD3ZuU"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(\"bert-base-amharic-model.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd3d1506",
      "metadata": {},
      "source": [
        "| Epoch | Training Loss | Validation Loss | Precision | Recall | F1 | Accuracy |\n",
        "| --- | --- | --- | --- | --- | --- | --- |\n",
        "| 1 | No log | 1.572634 | 0.000000 | 0.000000 | 0.000000 | 0.455782 |\n",
        "| 2 | No log | 1.271012 | 0.000000 | 0.000000 | 0.000000 | 0.591837 |\n",
        "| 3 | No log | 1.060682 | 0.727273 | 0.266667 | 0.390244 | 0.687075 |\n",
        "| 4 | No log | 0.893102 | 0.473684 | 0.300000 | 0.367347 | 0.727891 |\n",
        "| 5 | No log | 0.790093 | 0.600000 | 0.400000 | 0.480000 | 0.761905 |\n",
        "| 6 | No log | 0.752542 | 0.600000 | 0.400000 | 0.480000 | 0.761905 |\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
